{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's load the Reddit API creds.\n",
    "%load_ext dotenv\n",
    "%dotenv ../ingest/praw_creds.env\n",
    "%dotenv ../.env\n",
    "\n",
    "import os\n",
    "\n",
    "REDDIT_CLIENT_ID = os.environ.get(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.environ.get(\"REDDIT_CLIENT_SECRET\")\n",
    "SUBREDDIT = \"politics\"\n",
    "VERSION = os.environ.get(\"VERSION\") + \"-eda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncpraw\n",
    "\n",
    "\"\"\"\n",
    "Set up the Reddit client instance.\n",
    "I'm using a read-only PRAW intance because I have no need to post comments.\n",
    "I'm using async PRAW pretty much just because `ingest` does so.\n",
    "\n",
    "Credentials need to be supplied via env var.\n",
    "\"\"\"\n",
    "reddit = asyncpraw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=f\"python:vivshaw/politeiamancer:{VERSION} (by /u/vivshaw)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's load some comments!\n",
    "\"\"\"\n",
    "subreddit = await reddit.subreddit(SUBREDDIT)\n",
    "\n",
    "comments = []\n",
    "\n",
    "async for comment in subreddit.comments(limit=100):\n",
    "    comment_as_dict = {\n",
    "        # ID\n",
    "        \"fullname\": comment.name,\n",
    "        # Comment details\n",
    "        \"author\": comment.author,\n",
    "        \"body\": comment.body,\n",
    "        \"permalink\": comment.permalink,\n",
    "        # Time\n",
    "        \"created_utc\": int(comment.created_utc),\n",
    "    }\n",
    "    comments.append(comment_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Now that we've loaded them, we need to get 'em into a Pandas dataframe.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.DataFrame(comments)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\"\"\"\n",
    "Time for some sentiment analysis!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sentiment_score(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate a sentiment score for a piece of text using VADER.\n",
    "    \"\"\"\n",
    "    sentiment_intensity_analyzer = SentimentIntensityAnalyzer()\n",
    "    valence_scores = sentiment_intensity_analyzer.polarity_scores(text)\n",
    "    return valence_scores\n",
    "\n",
    "\n",
    "ratings_df = df[\"body\"].apply(sentiment_score).apply(pd.Series)\n",
    "df = pd.concat([df, ratings_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's see some summary stats.\n",
    "\"\"\"\n",
    "\n",
    "df[[\"neg\", \"neu\", \"pos\", \"compound\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How 'bout looking at our most-negative, most-neutral, and most-positive comment?\n",
    "\"\"\"\n",
    "\n",
    "most_negative = df.loc[df[\"neg\"].idxmax()]\n",
    "print(\"Most negative comment:\")\n",
    "print(most_negative[\"body\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "most_neutral = df.loc[df[\"neu\"].idxmax()]\n",
    "print(\"Most neutral comment:\")\n",
    "print(most_neutral[\"body\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "most_positive = df.loc[df[\"pos\"].idxmax()]\n",
    "print(\"Most positive comment:\")\n",
    "print(most_positive[\"body\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "OK, how about by compound score?\n",
    "\"\"\"\n",
    "compound_most_negative = df.loc[df[\"compound\"].idxmin()]\n",
    "print(\"Most negative compound score comment:\")\n",
    "print(compound_most_negative[\"body\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "compound_most_positive = df.loc[df[\"compound\"].idxmax()]\n",
    "print(\"Most positive compound score comment:\")\n",
    "print(compound_most_positive[\"body\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Alright, seems like compound scores are the way to go. The raw scores are not that informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "\n",
    "\"\"\"\n",
    "Next up, we'll analyze some emotional valence.\n",
    "\"\"\"\n",
    "emotions = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"trust\"]\n",
    "\n",
    "\n",
    "def emotion_scores(text):\n",
    "    emotion_dict = {emotion: 0 for emotion in emotions}\n",
    "\n",
    "    affect_frequencies = NRCLex(text).affect_frequencies\n",
    "\n",
    "    for emotion in emotions:\n",
    "        if emotion in affect_frequencies:\n",
    "            emotion_dict[emotion] = affect_frequencies[emotion]\n",
    "\n",
    "    return emotion_dict\n",
    "\n",
    "\n",
    "emotions_df = df[\"body\"].apply(emotion_scores).apply(pd.Series)\n",
    "df = pd.concat([df, emotions_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's see some summary stats.\n",
    "\"\"\"\n",
    "df[emotions].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OK, most emotional comments?\n",
    "\"\"\"\n",
    "most_angry = df.loc[df[\"anger\"].idxmax()]\n",
    "print(\"Most angry comment:\")\n",
    "print(most_angry[\"body\"])\n",
    "\n",
    "most_disgusted = df.loc[df[\"disgust\"].idxmax()]\n",
    "print(\"Most disgusted comment:\")\n",
    "print(most_disgusted[\"body\"])\n",
    "\n",
    "most_fearful = df.loc[df[\"fear\"].idxmax()]\n",
    "print(\"Most fearful comment:\")\n",
    "print(most_fearful[\"body\"])\n",
    "\n",
    "most_joyful = df.loc[df[\"joy\"].idxmax()]\n",
    "print(\"Most joyful comment:\")\n",
    "print(most_joyful[\"body\"])\n",
    "\n",
    "most_sad = df.loc[df[\"sadness\"].idxmax()]\n",
    "print(\"Most sad comment:\")\n",
    "print(most_sad[\"body\"])\n",
    "\n",
    "most_surprised = df.loc[df[\"surprise\"].idxmax()]\n",
    "print(\"Most surprised comment:\")\n",
    "print(most_surprised[\"body\"])\n",
    "\n",
    "most_trustful = df.loc[df[\"trust\"].idxmax()]\n",
    "print(\"Most trustful comment:\")\n",
    "print(most_trustful[\"body\"])\n",
    "\n",
    "\n",
    "# Alright, emotional analysis is not amazingly accurate. But it's worth poking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's grab the comments mentioning Trump.\n",
    "\"\"\"\n",
    "bodies = df[\"body\"].str.lower()\n",
    "\n",
    "trump_filter = bodies.str.contains(\"trump\")\n",
    "trump_df = df.loc[trump_filter]\n",
    "trump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df[\n",
    "    [\n",
    "        \"compound\",\n",
    "        \"anger\",\n",
    "        \"disgust\",\n",
    "        \"fear\",\n",
    "        \"joy\",\n",
    "        \"sadness\",\n",
    "        \"surprise\",\n",
    "        \"trust\",\n",
    "    ]\n",
    "].agg([\"count\", \"min\", \"max\", \"mean\", \"median\", \"skew\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now, those referring to Biden.\n",
    "\"\"\"\n",
    "\n",
    "biden_filter = bodies.str.contains(\"biden\")\n",
    "biden_df = df.loc[biden_filter]\n",
    "biden_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_df[[\"compound\"] + emotions].agg(\n",
    "    [\"count\", \"min\", \"max\", \"mean\", \"median\", \"skew\", \"std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the emotional deltas look like?\n",
    "(\n",
    "    biden_df[[\"compound\"] + emotions].mean()\n",
    "    - trump_df[\n",
    "        [\"compound\", \"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"trust\"]\n",
    "    ].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.pairplot(data=df[[\"compound\"] + emotions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After a number of runs, seems like there's not any big patterns emerging that I'd want to build a model for.\n",
    "# That's OK- my main goal is just to stream and visualize the data anyways!\n",
    "# What I think I _have_ seen is:\n",
    "#  - Biden results seem to have consistently higher skewness than Trump, as if Biden has a fatter positive-sentiment tail or is skewed by negative outliers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
